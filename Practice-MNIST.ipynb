{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "advised-chart",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eastern-condition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-affect",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "color-beach",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset = tfds.load(name='mnist') ## loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "resident-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info = True, as_supervised=True) # Getting additional information from the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "appropriate-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test'] # Splitting the dataset into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "supported-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples # Extracting 10% samples from train datasets for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "official-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64) \n",
    "#tf method suitable for casting variables into different data types\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "legendary-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image,label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image,label\n",
    "\n",
    "# build scale function as input for tensorflow map\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "BUFFER_SIZE = 10000 #needs to be greater than number of samples for uniform shuffling\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples) #skip the validation dataset\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data)) # Go through the validation data element by element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "satisfied-domain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 784 input nodes, 50 hidden nodes in two layers each, 10 output layers\n",
    "input_size = 784 # Each observation is 28 by 28 pixels (Tensor of rank 3 : 28 by 28 by 1)\n",
    "output_size = 10\n",
    "hidden_layer_size = 200 # all hidden layers are assumed to be of the same size\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)), # Reducing rank by flattening one dimension\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # Building each consecutive layer (an equivalent of the input and weight dot product and adding bias, also can be used to apply activation function)\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='tanh'), # Add as many hidden layers as needed\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax') # Use softmax to form output as probability\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-peace",
   "metadata": {},
   "source": [
    "## Optimization Algorithm (Loss Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "neither-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.02)\n",
    "model.compile(optimizer=custom_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy']) # sparse_categorical_crossentropy automatically incorporates one-hot encoding\n",
    "# Output metrics can be specified to monitor i.e. accuracy in this case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-pantyhose",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "warming-given",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 3s - loss: 0.3695 - accuracy: 0.8953 - val_loss: 0.2208 - val_accuracy: 0.9340\n",
      "Epoch 2/5\n",
      "540/540 - 3s - loss: 0.2586 - accuracy: 0.9283 - val_loss: 0.2197 - val_accuracy: 0.9377\n",
      "Epoch 3/5\n",
      "540/540 - 3s - loss: 0.2728 - accuracy: 0.9268 - val_loss: 0.3388 - val_accuracy: 0.8982\n",
      "Epoch 4/5\n",
      "540/540 - 3s - loss: 0.3026 - accuracy: 0.9173 - val_loss: 0.2741 - val_accuracy: 0.9248\n",
      "Epoch 5/5\n",
      "540/540 - 3s - loss: 0.3288 - accuracy: 0.9160 - val_loss: 0.2965 - val_accuracy: 0.9203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x269bc9cc040>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "model.fit(train_data, epochs = NUM_EPOCHS, validation_data = (validation_inputs, validation_targets), verbose = 2)\n",
    "\n",
    "# Training loss = 0 at the beginning of each epoch\n",
    "# The algorithm will iterate over a set number of batches all from train_data\n",
    "# Updating weights and biases as many times as the number of batches\n",
    "# However loss function updated only after each epoch once all batches pass through the training at least once i.e = number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-comedy",
   "metadata": {},
   "source": [
    "Both validation and training loss are decreasing hence we are increasing in accuracy but not overfitting. Doubling the hidden layer size from 50 to 100 has increased the accuracy. Doubling further increases accuracy but not so much. Adding a hidden layer increases the accuracy to 98.5%. With 5 hidden layers, the accuracy does not improve. Using two hidden layers each with a sigmoid activation drops the accuracy back to 97%. Using two hidden layers, one with a relu activation function and the other with a tanh activation function increases the accuracy upto 98.9%. Changing the batch size from 100 to 10000 causes a loss of accuracy which drops to 90.6%. Using 1 as batch size significantly increases the learning time where all trainings before were completed withing 7-8 secs, with a batch size of 1, each epoch takes approximately 53-58 secs and the accuracy is 96.8% so using SGD is not the best option.\n",
    "\n",
    "Using a custom learning rate of 0.0001 gives an accuracy of 95.7% with a total training time of 10s. Adjusting the learning rate to 0.02 gives an accuracy of 92% with a total learning time of 12s. Best to use the default value in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-making",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
